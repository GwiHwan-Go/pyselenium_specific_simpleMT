{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jieba\n",
    "# !pip install emoji\n",
    "import jieba\n",
    "def segmentate_sentence(sent):\n",
    "    \"\"\"\n",
    "    word segmentation with jieba\n",
    "    ref : https://github.com/fxsjy/jieba\n",
    "    Args :\n",
    "        sent(Str) : a sentence without segmentation, e.g \"Iloveyou\"\n",
    "    Output :\n",
    "        sent(str) : segmentated sentence, e.g \"I love you\"\n",
    "    \"\"\"\n",
    "    # sent = clean(sent)\n",
    "    seg_list = jieba.lcut(sent, cut_all=False)\n",
    "    \n",
    "    seg_list = [i for i in seg_list if i!=\" \"]\n",
    "    return seg_list\n",
    "    \n",
    "def clean(x):\n",
    "    \"\"\"clean tweets\n",
    "    :param: a tweet (str)\n",
    "    :return a preprocessed tweet (str)\n",
    "    \"\"\"\n",
    "    import emoji\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "    username_pattern = re.compile(\n",
    "        r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\")\n",
    "    url_pattern = re.compile(\n",
    "        r\"[-a-zA-Z0-9@:%_\\+.~#?&//=]{2,256}\\.[a-z]{2,4}\\b(\\/[-a-zA-Z0-9@:%_\\+.~#?&//=]*)?\")\n",
    "\n",
    "    chinese_pattern=re.compile(r\"[^\\u4e00-\\u9fa5]+\")\n",
    "#     x = username_pattern.sub('', x)\n",
    "#     x = url_pattern.sub('', x)\n",
    "    x = chinese_pattern.sub('', x)\n",
    "#     x = re.sub(r'[0-9]+', '', x)\n",
    "#     table = x.maketrans({ '/': ' ',  '#': ' ', '@':' ', ':':' '})\n",
    "#     x = x.translate(table)\n",
    "    x = re.sub(pattern, '', x)\n",
    "#     x = re.sub(r\"[^a-zA-Z0-9]\",\"\",x) # 특수문자 제거\n",
    "    x = x.strip()\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from xlsx to csv\n",
    "def make_user_dict(xlsx_path, path) :\n",
    "    \"\"\"\n",
    "    make a custom dictionary for jieba\n",
    "    It makes a txt file to communicate with jieba\n",
    "    xlsx_path : A path of dictionary\n",
    "    path : A result path of dictionary\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    with open(path, \"w\") as f :\n",
    "        f.writelines('\\n'.join(df.loc[:,'words']))\n",
    "    f.close()\n",
    "    \n",
    "def add_words(raw_paths, existing_data=None) :\n",
    "    \"\"\"\n",
    "    add new data with existing data\n",
    "    input :\n",
    "        raw_paths : list of path in train_data folder\n",
    "        existing_data : dataframe - existing data, if not given, it initalize the dataset\n",
    "    output :\n",
    "        set of words\n",
    "    \"\"\"\n",
    "    # # !pip install googletrans==3.1.0a0\n",
    "    # # ref) https://stackoverflow.com/questions/52455774/googletrans-stopped-working-with-error-nonetype-object-has-no-attribute-group\n",
    "    # # ref) https://pypi.org/project/googletrans/\n",
    "    # from googletrans import Translator\n",
    "    # gmt = Translator()\n",
    "    # gmt.translate(word, dest='ko').text\n",
    "\n",
    "    bag_of_words = []\n",
    "    cnt_words = []\n",
    "    meanings = []\n",
    "    positions = []\n",
    "\n",
    "    if existing_data is not None :\n",
    "        bag_of_words = list(existing_data.loc[:,'words'])\n",
    "        cnt_words = list(existing_data.loc[:,'cnt'])\n",
    "        meanings = list(existing_data.loc[:,'meaning'])\n",
    "        positions = list(existing_data.loc[:,'position'])\n",
    "\n",
    "    for path in raw_paths :\n",
    "        df = pd.read_csv(f\"train_data/{path}.csv\")\n",
    "\n",
    "        for row in tqdm(range(df.shape[0])) :\n",
    "            for word in segmentate_sentence(df['English title'][row]) :\n",
    "                if word in bag_of_words :\n",
    "                    cnt_words[bag_of_words.index(word)]+=1\n",
    "                else :\n",
    "                    bag_of_words.append(word)\n",
    "                    cnt_words.append(1)\n",
    "                    res = translate(word)\n",
    "                    meanings.append(json.loads(res)['message']['result']['translatedText'])\n",
    "                    positions.append(\"\")\n",
    "\n",
    "    df = pd.DataFrame(list(zip(bag_of_words, meanings, positions, cnt_words)),columns =['words', 'meaning', 'position','cnt'])\n",
    "    return df.sort_values(by='cnt', ascending=False).reset_index(drop=True)\n",
    "\n",
    "def translate(target) :\n",
    "    import os\n",
    "    import sys\n",
    "    import urllib.request\n",
    "    # from Local import client_id, client_secret\n",
    "    client_id = \"sy3GuTVVRhzvp2xC8vVM\" # 개발자센터에서 발급받은 Client ID 값\n",
    "    client_secret = \"D1CqkDNhAq\" \n",
    "    encText = urllib.parse.quote(target)\n",
    "    data = \"source=zh-CN&target=ko&text=\" + encText\n",
    "    url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    response = urllib.request.urlopen(request, data=data.encode(\"utf-8\"))\n",
    "    rescode = response.getcode()\n",
    "    if(rescode==200):\n",
    "        response_body = response.read()\n",
    "        return response_body.decode('utf-8')\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "\n",
    "\n",
    "# import json\n",
    "# res = translate(\"娃娃领\")\n",
    "# json.loads(res)['message']['result']['translatedText']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall googletrans==4.0.0-rc1\n",
    "\n",
    "###Make it as word and save on table###\n",
    "###table_col : mymeaning, MT_meaning, count, position\n",
    "\n",
    "#########TRAIN SECTION##########\n",
    "make_user_dict(\"./train_data/reference.xlsx\",\"./train_data/userdict.txt\")\n",
    "jieba.load_userdict(\"./train_data/userdict.txt\") # load customized dictionary\n",
    "reference= pd.read_excel(\"./train_data/reference.xlsx\")\n",
    "reference = reference.set_index('words', drop=True)\n",
    "#########TRAIN SECTION##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_data/6_5.csv\")\n",
    "sentences = [None]*df.shape[0]\n",
    "for row in range(df.shape[0]) :\n",
    "    sentences[row] = segmentate_sentence(df['English title'][row])\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\t2022\n",
      "春夏\t봄여름\n",
      "新款\t신상\n",
      "女装\t여성복\n",
      "时尚\t트랜드한\n",
      "chic\tchic\n",
      "甜美\t감미로운\n",
      "学院风\t캠퍼스\n",
      "长裙\t롱스커트\n",
      "套装\t세트\n",
      "牛仔\t데님\n",
      "背带\t멜빵\n",
      "连衣裙\t원피스\n",
      "子夏\t자하\n"
     ]
    }
   ],
   "source": [
    "with open(\"./new_words.txt\", \"w\") as f :\n",
    "    for word in sentences[1] :\n",
    "        if word in reference.index :\n",
    "            print(word, end='\\t')\n",
    "            print(reference.loc[word, 'meaning'])\n",
    "        else :\n",
    "            print(\"no existing word\")\n",
    "            f.write(word)\n",
    "            f.write(\"\\n\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - One hot encoding\n",
    "\n",
    "One hot encoding including position infos.\n",
    "\n",
    "meaning, category, position info\n",
    "\n",
    "1. save on csv - with checking if there is this word\n",
    "2. add meaning\n",
    "3. add position meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words - word - meaning - cate - \n",
    "1\n",
    "2\n",
    "3\n",
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Opn other files and save on this table too###\n",
    "###No overlap"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95cb90694882dadca987f5fb6182f3bb000d75c3a77cb8ddf9893f9283882476"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('py_selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
