{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jieba\n",
    "# !pip install emoji\n",
    "import jieba\n",
    "def segmentate_sentence(sent):\n",
    "    \"\"\"\n",
    "    word segmentation with jieba\n",
    "    ref : https://github.com/fxsjy/jieba\n",
    "    Args :\n",
    "        sent(Str) : a sentence without segmentation, e.g \"Iloveyou\"\n",
    "    Output :\n",
    "        sent(str) : segmentated sentence, e.g \"I love you\"\n",
    "    \"\"\"\n",
    "    # sent = clean(sent)\n",
    "    seg_list = jieba.lcut(sent, cut_all=False)\n",
    "    \n",
    "    seg_list = [i for i in seg_list if i!=\" \"]\n",
    "    return seg_list\n",
    "    \n",
    "def clean(x):\n",
    "    \"\"\"clean tweets\n",
    "    :param: a tweet (str)\n",
    "    :return a preprocessed tweet (str)\n",
    "    \"\"\"\n",
    "    import emoji\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "    username_pattern = re.compile(\n",
    "        r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\")\n",
    "    url_pattern = re.compile(\n",
    "        r\"[-a-zA-Z0-9@:%_\\+.~#?&//=]{2,256}\\.[a-z]{2,4}\\b(\\/[-a-zA-Z0-9@:%_\\+.~#?&//=]*)?\")\n",
    "\n",
    "    chinese_pattern=re.compile(r\"[^\\u4e00-\\u9fa5]+\")\n",
    "#     x = username_pattern.sub('', x)\n",
    "#     x = url_pattern.sub('', x)\n",
    "    x = chinese_pattern.sub('', x)\n",
    "#     x = re.sub(r'[0-9]+', '', x)\n",
    "#     table = x.maketrans({ '/': ' ',  '#': ' ', '@':' ', ':':' '})\n",
    "#     x = x.translate(table)\n",
    "    x = re.sub(pattern, '', x)\n",
    "#     x = re.sub(r\"[^a-zA-Z0-9]\",\"\",x) # 특수문자 제거\n",
    "    x = x.strip()\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from xlsx to csv\n",
    "def make_user_dict(xlsx_path, path) :\n",
    "    \"\"\"\n",
    "    make a custom dictionary for jieba\n",
    "    It makes a txt file to communicate with jieba\n",
    "    xlsx_path : A path of dictionary\n",
    "    path : A result path of dictionary\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    with open(path, \"w\") as f :\n",
    "        f.writelines('\\n'.join(df.loc[:,'words']))\n",
    "    f.close()\n",
    "    \n",
    "def add_words(raw_paths, existing_data=None) :\n",
    "    \"\"\"\n",
    "    add new data with existing data\n",
    "    input :\n",
    "        raw_paths : list of path in train_data folder\n",
    "        existing_data : dataframe - existing data, if not given, it initalize the dataset\n",
    "    output :\n",
    "        set of words\n",
    "    \"\"\"\n",
    "    # # !pip install googletrans==3.1.0a0\n",
    "    # # ref) https://stackoverflow.com/questions/52455774/googletrans-stopped-working-with-error-nonetype-object-has-no-attribute-group\n",
    "    # # ref) https://pypi.org/project/googletrans/\n",
    "    # from googletrans import Translator\n",
    "    # gmt = Translator()\n",
    "    # gmt.translate(word, dest='ko').text\n",
    "\n",
    "    bag_of_words = []\n",
    "    cnt_words = []\n",
    "    meanings = []\n",
    "    positions = []\n",
    "\n",
    "    if existing_data is not None :\n",
    "        bag_of_words = list(existing_data.loc[:,'words'])\n",
    "        cnt_words = list(existing_data.loc[:,'cnt'])\n",
    "        meanings = list(existing_data.loc[:,'meaning'])\n",
    "        positions = list(existing_data.loc[:,'position'])\n",
    "\n",
    "    for path in raw_paths :\n",
    "        df = pd.read_csv(f\"train_data/{path}.csv\")\n",
    "\n",
    "        for row in tqdm(range(df.shape[0])) :\n",
    "            for word in segmentate_sentence(df['English title'][row]) :\n",
    "                if word in bag_of_words :\n",
    "                    cnt_words[bag_of_words.index(word)]+=1\n",
    "                else :\n",
    "                    bag_of_words.append(word)\n",
    "                    cnt_words.append(1)\n",
    "                    res = translate(word)\n",
    "                    meanings.append(json.loads(res)['message']['result']['translatedText'])\n",
    "                    positions.append(\"\")\n",
    "\n",
    "    df = pd.DataFrame(list(zip(bag_of_words, meanings, positions, cnt_words)),columns =['words', 'meaning', 'position','cnt'])\n",
    "    return df.sort_values(by='cnt', ascending=False).reset_index(drop=True)\n",
    "\n",
    "def translate(target) :\n",
    "    import os\n",
    "    import sys\n",
    "    import urllib.request\n",
    "    # from Local import client_id, client_secret\n",
    "    client_id = \"sy3GuTVVRhzvp2xC8vVM\" # 개발자센터에서 발급받은 Client ID 값\n",
    "    client_secret = \"D1CqkDNhAq\" \n",
    "    encText = urllib.parse.quote(target)\n",
    "    data = \"source=zh-CN&target=ko&text=\" + encText\n",
    "    url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    response = urllib.request.urlopen(request, data=data.encode(\"utf-8\"))\n",
    "    rescode = response.getcode()\n",
    "    if(rescode==200):\n",
    "        response_body = response.read()\n",
    "        return response_body.decode('utf-8')\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "\n",
    "\n",
    "# import json\n",
    "# res = translate(\"娃娃领\")\n",
    "# json.loads(res)['message']['result']['translatedText']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/jt/jg8f8t5d5l94gkq96yn3k41h0000gn/T/jieba.cache\n",
      "Loading model cost 0.927 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall googletrans==4.0.0-rc1\n",
    "\n",
    "###Make it as word and save on table###\n",
    "###table_col : mymeaning, MT_meaning, count, position\n",
    "\n",
    "#########TRAIN SECTION##########\n",
    "make_user_dict(\"./train_data/reference.xlsx\",\"./train_data/userdict.txt\")\n",
    "jieba.load_userdict(\"./train_data/userdict.txt\") # load customized dictionary\n",
    "reference= pd.read_excel(\"./train_data/reference.xlsx\")\n",
    "reference = reference.set_index('words', drop=True)\n",
    "#########TRAIN SECTION##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "曾小咸\n",
      "黑标\n",
      "钢圈\n",
      "乞丐\n",
      "木耳边\n",
      "你好\n",
      "卡农\n",
      "落落狷介\n",
      "觅定\n",
      "阔腿\n",
      "直筒裤\n",
      "波点\n",
      "牛角扣\n",
      "小个子\n",
      "蓝\n",
      "纯欲\n",
      "灯笼袖\n",
      "单\n",
      "排扣\n",
      "超仙\n",
      "气质\n",
      "通勤\n",
      "特大码\n",
      "透视\n",
      "雪中飞\n",
      "data has been updated 0 times\n"
     ]
    }
   ],
   "source": [
    "## Retrain\n",
    "## Retrain model with ./working/new_data.txt\n",
    "\n",
    "def retrain(txt_path, ref_path) :\n",
    "    \"\"\"\n",
    "    update the dictionary with txt file.\n",
    "    if line startswith '`' : update with this\n",
    "    input :\n",
    "        txt_path : updated contents\n",
    "        ref_path : reference dictionary's path\n",
    "    \"\"\"\n",
    "\n",
    "    dic = defaultdict(str)\n",
    "    reference= pd.read_excel(ref_path)\n",
    "    reference = reference.set_index('words', drop=True)\n",
    "    changed=[]\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        li = f.readlines()\n",
    "        li = [i.strip() for i in li]\n",
    "\n",
    "    for word in li :\n",
    "        if word.startswith('---') or word.startswith('result'):\n",
    "            continue\n",
    "        if word.startswith('₩'):\n",
    "            try :\n",
    "                key, item = word.split(',')\n",
    "                dic[key.strip()[1:]] = item.strip()\n",
    "            except :\n",
    "                pass\n",
    "\n",
    "    for key in dic.keys() :\n",
    "        print(key)\n",
    "        if key in reference.index :\n",
    "            if reference.loc[key, 'meaning'] != dic[key] :\n",
    "                reference.loc[key, 'meaning'] = dic[key]\n",
    "                changed.append(key)\n",
    "        else :\n",
    "            reference.loc[key] = [dic[key], 0,1]\n",
    "            changed.append(key)\n",
    "    for key in changed :            \n",
    "        print(f\"{key} -> {reference.loc[key]['meaning']}\")\n",
    "\n",
    "    print(f\"data has been updated {len(changed)} times\")\n",
    "\n",
    "    reference.to_excel(ref_path)  \n",
    "\n",
    "retrain(\"./working/new_data.txt\", \"./train_data/reference.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:36<00:00,  2.74it/s]\n"
     ]
    }
   ],
   "source": [
    "##Start Working##\n",
    "###蓝语 왜 못찾는거야???? 확인, -> reference 에는 있는데 못찾아서 새로 찾는듯.\n",
    "df = pd.read_csv(\"working/work.csv\")\n",
    "#########LOAD TRAIN SECTION##########\n",
    "make_user_dict(\"./train_data/reference.xlsx\",\"./train_data/userdict.txt\")\n",
    "jieba.load_userdict(\"./train_data/userdict.txt\") # load customized dictionary\n",
    "reference= pd.read_excel(\"./train_data/reference.xlsx\")\n",
    "reference = reference.set_index('words', drop=True)\n",
    "reference = reference.dropna(axis=0)\n",
    "#########LOAD TRAIN SECTION##########\n",
    "with open(\"./working/auto.txt\", \"w\") as h :\n",
    "    with open(\"./working/new_data.txt\", \"w\")as g :\n",
    "        with open(\"./new_words.txt\", \"w\") as f :\n",
    "            for row in tqdm(range(df.shape[0])) :\n",
    "                sentence = segmentate_sentence(df['English title'][row])\n",
    "                translated = []\n",
    "                buff = defaultdict(str)\n",
    "                g.write(f\"\\n-------------------{row+1}------------\\n\")\n",
    "                h.write(f\"{row+1}.\\n\")\n",
    "                h.write(f\"{' '.join(sentence)}\\n\")\n",
    "                for word in segmentate_sentence(df['English title'][row]) :\n",
    "                    if word in reference.index :\n",
    "                        buff[word] = reference.loc[word, 'meaning']\n",
    "                        translated.append(str(reference.loc[word, 'meaning']))\n",
    "                    else :\n",
    "                        res = translate(word)\n",
    "                        res = json.loads(res)['message']['result']['translatedText']\n",
    "                        # res = word\n",
    "                        buff[word] = res\n",
    "                        translated.append(res)\n",
    "                        f.write(f\"{word}, {res}\")\n",
    "                        f.write(\"\\n\")\n",
    "                h.write(f\"{' '.join(translated)}\\n\")\n",
    "                for key, item in buff.items() :\n",
    "                    g.write(f\"{key}, {item}\\n\")\n",
    "        f.close()\n",
    "    g.close()\n",
    "h.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference.to_excel(\"./train_data/reference.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating - word\n",
    "\n",
    "One hot encoding including position infos.\n",
    "\n",
    "meaning, category, position info\n",
    "\n",
    "1. give main meaning\n",
    "2. add meaning\n",
    "3. add position meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words - word - meaning - cate - \n",
    "1\n",
    "2\n",
    "3\n",
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Opn other files and save on this table too###\n",
    "###No overlap"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95cb90694882dadca987f5fb6182f3bb000d75c3a77cb8ddf9893f9283882476"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('py_selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
