{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwihwango/.pyenv/versions/3.7.3/envs/py_selenium/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jieba\n",
    "# !pip install emoji\n",
    "import jieba\n",
    "def segmentate_sentence(sent):\n",
    "    \"\"\"\n",
    "    word segmentation with jieba\n",
    "    ref : https://github.com/fxsjy/jieba\n",
    "    Args :\n",
    "        sent(Str) : a sentence without segmentation, e.g \"Iloveyou\"\n",
    "    Output :\n",
    "        sent(str) : segmentated sentence, e.g \"I love you\"\n",
    "    \"\"\"\n",
    "    # sent = clean(sent)\n",
    "    seg_list = jieba.lcut(sent, cut_all=False)\n",
    "    \n",
    "    seg_list = [i for i in seg_list if i!=\" \"]\n",
    "    return seg_list\n",
    "    \n",
    "def clean(x):\n",
    "    \"\"\"clean tweets\n",
    "    :param: a tweet (str)\n",
    "    :return a preprocessed tweet (str)\n",
    "    \"\"\"\n",
    "    import emoji\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "    username_pattern = re.compile(\n",
    "        r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\")\n",
    "    url_pattern = re.compile(\n",
    "        r\"[-a-zA-Z0-9@:%_\\+.~#?&//=]{2,256}\\.[a-z]{2,4}\\b(\\/[-a-zA-Z0-9@:%_\\+.~#?&//=]*)?\")\n",
    "\n",
    "    chinese_pattern=re.compile(r\"[^\\u4e00-\\u9fa5]+\")\n",
    "#     x = username_pattern.sub('', x)\n",
    "#     x = url_pattern.sub('', x)\n",
    "    x = chinese_pattern.sub('', x)\n",
    "#     x = re.sub(r'[0-9]+', '', x)\n",
    "#     table = x.maketrans({ '/': ' ',  '#': ' ', '@':' ', ':':' '})\n",
    "#     x = x.translate(table)\n",
    "    x = re.sub(pattern, '', x)\n",
    "#     x = re.sub(r\"[^a-zA-Z0-9]\",\"\",x) # 특수문자 제거\n",
    "    x = x.strip()\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Retrain Started#####\n",
      "data has been updated 0 times\n",
      "#####Retrain Ended#####\n"
     ]
    }
   ],
   "source": [
    "#from xlsx to csv\n",
    "def make_user_dict(xlsx_path, path) :\n",
    "    \"\"\"\n",
    "    make a custom dictionary for jieba\n",
    "    It makes a txt file to communicate with jieba\n",
    "    xlsx_path : A path of dictionary\n",
    "    path : A result path of dictionary\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    with open(path, \"w\") as f :\n",
    "        f.writelines('\\n'.join(df.loc[:,'words']))\n",
    "    f.close()\n",
    "    \n",
    "def add_words(raw_paths, existing_data=None) :\n",
    "    \"\"\"\n",
    "    add new data with existing data\n",
    "    input :\n",
    "        raw_paths : list of path in train_data folder\n",
    "        existing_data : dataframe - existing data, if not given, it initalize the dataset\n",
    "    output :\n",
    "        set of words\n",
    "    \"\"\"\n",
    "    # # !pip install googletrans==3.1.0a0\n",
    "    # # ref) https://stackoverflow.com/questions/52455774/googletrans-stopped-working-with-error-nonetype-object-has-no-attribute-group\n",
    "    # # ref) https://pypi.org/project/googletrans/\n",
    "    # from googletrans import Translator\n",
    "    # gmt = Translator()\n",
    "    # gmt.translate(word, dest='ko').text\n",
    "\n",
    "    bag_of_words = []\n",
    "    cnt_words = []\n",
    "    meanings = []\n",
    "    positions = []\n",
    "\n",
    "    if existing_data is not None :\n",
    "        bag_of_words = list(existing_data.loc[:,'words'])\n",
    "        cnt_words = list(existing_data.loc[:,'cnt'])\n",
    "        meanings = list(existing_data.loc[:,'meaning'])\n",
    "        positions = list(existing_data.loc[:,'position'])\n",
    "\n",
    "    for path in raw_paths :\n",
    "        df = pd.read_csv(f\"train_data/{path}.csv\")\n",
    "\n",
    "        for row in tqdm(range(df.shape[0])) :\n",
    "            for word in segmentate_sentence(df['English title'][row]) :\n",
    "                if word in bag_of_words :\n",
    "                    cnt_words[bag_of_words.index(word)]+=1\n",
    "                else :\n",
    "                    bag_of_words.append(word)\n",
    "                    cnt_words.append(1)\n",
    "                    res = translate(word)\n",
    "                    meanings.append(json.loads(res)['message']['result']['translatedText'])\n",
    "                    positions.append(\"\")\n",
    "\n",
    "    df = pd.DataFrame(list(zip(bag_of_words, meanings, positions, cnt_words)),columns =['words', 'meaning', 'position','cnt'])\n",
    "    return df.sort_values(by='cnt', ascending=False).reset_index(drop=True)\n",
    "\n",
    "def translate(target) :\n",
    "    import os\n",
    "    import sys\n",
    "    import urllib.request\n",
    "    # from Local import client_id, client_secret\n",
    "    client_id = \"sy3GuTVVRhzvp2xC8vVM\" # 개발자센터에서 발급받은 Client ID 값\n",
    "    client_secret = \"D1CqkDNhAq\" \n",
    "    encText = urllib.parse.quote(target)\n",
    "    data = \"source=zh-CN&target=ko&text=\" + encText\n",
    "    url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    response = urllib.request.urlopen(request, data=data.encode(\"utf-8\"))\n",
    "    rescode = response.getcode()\n",
    "    if(rescode==200):\n",
    "        response_body = response.read()\n",
    "        return response_body.decode('utf-8')\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "\n",
    "def retrain(txt_path, ref_path) :\n",
    "    \"\"\"\n",
    "    update the dictionary with txt file.\n",
    "    if line startswith '`' : update with this\n",
    "    input :\n",
    "        txt_path : updated contents\n",
    "        ref_path : reference dictionary's path\n",
    "    \"\"\"\n",
    "    print(\"#####Retrain Started#####\")\n",
    "    dic = defaultdict(str)\n",
    "    position = defaultdict(str)\n",
    "    reference= pd.read_excel(ref_path)\n",
    "    reference = reference.set_index('words', drop=True)\n",
    "    changed=[]\n",
    "    cnt=0\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        li = f.readlines()\n",
    "        li = [i.strip() for i in li]\n",
    "\n",
    "    for word in li :\n",
    "        if not word.startswith('---') and len(word.strip())>1 :\n",
    "            if word.endswith('₩') :\n",
    "                word = word[:-1]\n",
    "                key, item, pos = word.split('|')\n",
    "                dic[key.strip()] = item.strip()\n",
    "                position[key.strip()] = pos.strip()\n",
    "    for key in dic.keys() :\n",
    "        if key in reference.index :\n",
    "            if reference.loc[key, 'meaning'] != dic[key] :\n",
    "                print(f\"{key} : {reference.loc[key, 'meaning']}->\", end=\"\")\n",
    "                reference.loc[key, 'meaning'] = dic[key]\n",
    "                print(f\"{reference.loc[key, 'meaning']}\")\n",
    "                cnt+=1\n",
    "            if reference.loc[key, 'position'] != position[key] :\n",
    "                print(f\"{key} : {reference.loc[key, 'position']}->\", end=\"\")\n",
    "                reference.loc[key, 'position'] = position[key]\n",
    "                print(f\"{reference.loc[key, 'position']}\")\n",
    "                cnt+=1\n",
    "\n",
    "        else :\n",
    "            print(key)\n",
    "            if position[key] != 'None' :\n",
    "                reference.loc[key] = [dic[key], position[key],1]\n",
    "                print(f\"{key} : {list(reference.loc[key, :])} \")\n",
    "                cnt+=1\n",
    "\n",
    "\n",
    "    print(f\"data has been updated {cnt} times\")\n",
    "    reference.to_excel(ref_path)  \n",
    "    print(\"#####Retrain Ended#####\")\n",
    "\n",
    "    return reference\n",
    "\n",
    "# reference= pd.read_excel(\"./train_data/reference.xlsx\")\n",
    "# reference = reference.set_index('words', drop=True)\n",
    "# reference = retrain(\"./working/new_data.txt\", \"./train_data/reference.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上衣 상의 --> 1\n",
      "外套 재킷 --> 1\n",
      "T恤 티셔츠 --> 1\n",
      "裤 팬츠 --> 1\n",
      "裤子 팬츠 --> 1\n",
      "牛仔裤 청바지 --> 1\n",
      "针织 니트 --> 1\n",
      "长裤 긴 바지 --> 1\n",
      "衫 셔츠 --> 1\n",
      "衬衣 셔츠 --> 1\n",
      "短裤 반바지 --> 1\n",
      "针织衫 니트 --> 1\n",
      "西装裤 정장 바지 --> 1\n",
      "休闲裤 캐주얼 바지 --> 1\n",
      "运动裤 트레이닝 바지 --> 1\n",
      "风衣 트렌치코트 --> 1\n",
      "卫裤 맨투맨 바지 --> 1\n",
      "短外套 쇼트 재킷 --> 1\n",
      "连帽 후드 --> 1\n",
      "筒裤 일자 바지 --> 1\n",
      "背带裤 멜빵 바지 --> 1\n",
      "阔腿裤 와이드 팬츠 --> 1\n",
      "体恤 티셔츠 --> 1\n",
      "底裤 속바지 --> 1\n",
      "喇叭裤 나팔 바지 --> 1\n",
      "夹克 재킷 --> 1\n",
      "呢子大衣 모직 코트 --> 1\n",
      "工装裤 카고 바지 --> 1\n",
      "西裤 양복 바지 --> 1\n",
      "热裤 핫팬츠 --> 1\n",
      "裤型 바지 모양 --> 1\n",
      "裤加绒 바지 기모 --> 1\n",
      "裤裙 치마바지 --> 1\n",
      "微喇裤 부츠컷 팬츠 --> 1\n",
      "帽卫衣 후드 맨투맨 --> 1\n",
      "裤卫裤 팬츠 바지 --> 1\n",
      "裤外 바지 밖 --> 1\n",
      "五分裤 5부 팬츠 --> 1\n",
      "白衬衫 화이트 셔츠 --> 1\n",
      "裤百搭女 바지저고리녀 --> 1\n",
      "牛仔衣 데님 셔츠 --> 1\n",
      "灯笼裤 벌룬팬츠 --> 1\n",
      "女上衣 여자 상의 --> 1\n",
      "貂皮大衣 밍크 코트 --> 1\n",
      "t恤 티셔츠 --> 1\n",
      "上衣服 상의 --> 1\n",
      "单色T恤 무지티셔츠 --> 1\n",
      "开衫卫衣 후드집업 --> 1\n",
      "羊绒外套 캐시미어 코트 --> 1\n",
      "羊皮夹克 무스탕 재킷 --> 1\n",
      "毛呢连衣裙 니트 원피스 --> 1\n",
      "哈伦裤 배기 바지 --> 1\n",
      "钩花 플라워 니트 디자인 --> 1\n",
      "直筒裤 일자 바지 --> 1\n"
     ]
    }
   ],
   "source": [
    "#######Add position of words which are in the given set#####\n",
    "def get_key_words(txt_path) :\n",
    "    with open(txt_path, \"r\") as f :\n",
    "        li = [i.strip() for i in f.readlines()]\n",
    "    return li\n",
    "\n",
    "def give_position(set_of_words, position, reference) :\n",
    "    \"\"\"\n",
    "    Add position information to reference file\n",
    "    \"\"\"\n",
    "    for idx in reference.index :\n",
    "        word = reference.loc[idx,'meaning']\n",
    "        word = str(word)\n",
    "        flag=False\n",
    "        for chaidx in range(len(str(word))) :\n",
    "            if word[chaidx] in set_of_words :\n",
    "                flag=True\n",
    "            if chaidx != len(word)-1 : ### You should renew here\n",
    "                if word[chaidx:chaidx+2] in set_of_words :\n",
    "                    flag=True\n",
    "        if flag :\n",
    "            reference.loc[idx,'position'] = position\n",
    "            print(idx, reference.loc[idx,'meaning'], \"-->\", position)\n",
    "    \n",
    "    return reference\n",
    "\n",
    "txt_path = \"./keywords/kinds.txt\"\n",
    "position = 1\n",
    "reference= pd.read_excel(\"./train_data/reference.xlsx\")\n",
    "reference = reference.set_index('words', drop=True)\n",
    "reference = reference.dropna(axis=0)\n",
    "reference = give_position(get_key_words(txt_path), position, reference)\n",
    "reference.to_excel(\"./train_data/reference.xlsx\")\n",
    "\n",
    "# for idx in reference.index :\n",
    "#     print(idx)\n",
    "#     try :\n",
    "#         if reference.loc[idx, 'position'] == 'None' :\n",
    "#             reference = reference.drop([idx])\n",
    "#     except :\n",
    "#         reference = reference.drop([idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小\n"
     ]
    }
   ],
   "source": [
    "reference= pd.read_excel(\"./train_data/reference.xlsx\")\n",
    "reference = reference.set_index('words', drop=True)\n",
    "reference = reference.dropna(axis=0)\n",
    "for idx in reference.index :\n",
    "    try :\n",
    "        if int(reference.loc[idx, 'position']) == 0 :\n",
    "            pass\n",
    "    except :\n",
    "        print(idx)\n",
    "        reference = reference.drop([idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference.to_excel(\"./train_data/reference.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Retrain Started#####\n",
      "若语\n",
      "若语 : ['Ruoyu', '0', 1] \n",
      "暗纹\n",
      "暗纹 : ['히든 디자인', '9', 1] \n",
      "瞌睡兔\n",
      "瞌睡兔 : ['Keshhuitu', '0', 1] \n",
      "韩风\n",
      "韩风 : ['한국 스타일', '9', 1] \n",
      "data has been updated 4 times\n",
      "#####Retrain Ended#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:11<00:00,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto.txt has successfully saved as ./working/auto.txt\n",
      "new_data.txt has successfully saved as ./working/new_data.txt\n",
      "Everything has completed, Go to work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##Start Working##\n",
    "def forward(data_path, train_data, user_dict, start) :\n",
    "    df = pd.read_csv(data_path)\n",
    "    #########LOAD TRAINED SECTION##########\n",
    "    make_user_dict(train_data, user_dict)\n",
    "    jieba.load_userdict(user_dict) # load customized dictionary\n",
    "    reference= pd.read_excel(train_data)\n",
    "    reference = reference.set_index('words', drop=True)\n",
    "    reference = reference.dropna(axis=0)\n",
    "    #########LOAD TRAINED SECTION##########\n",
    "    with open(\"./working/auto.txt\", \"w\") as h :\n",
    "        with open(\"./working/new_data.txt\", \"w\")as g :\n",
    "            for row in tqdm(range(start, df.shape[0])) :\n",
    "                ###################Generating Section###########################\n",
    "                sentence = segmentate_sentence(df['English title'][row])\n",
    "                translated = []\n",
    "                buff = defaultdict(str)\n",
    "                key_words= ['2022', '신상', '2022신상', '22', '20', '년', 2022] ## For Extra section\n",
    "                season=\"\"\n",
    "                new_flag=False\n",
    "                for word in segmentate_sentence(df['English title'][row]) :\n",
    "                    if word in reference.index :\n",
    "                        if int(reference.loc[word, 'position']) ==11 : ##Check Season\n",
    "                            season = reference.loc[word,'meaning']\n",
    "                        buff[word] = list(reference.loc[word, :])\n",
    "                        translated.append(str(buff[word][0]))\n",
    "                    else :\n",
    "                        res = translate(word)\n",
    "                        res = json.loads(res)['message']['result']['translatedText']\n",
    "                        buff[word] = [res,None,1]\n",
    "                        translated.append(buff[word][0])\n",
    "                ###################Generating Section###########################\n",
    "\n",
    "                ###################Extra Section###########################\n",
    "                    if new_flag==False :\n",
    "                        if buff[word][0] in key_words :        \n",
    "                            new_flag=True\n",
    "\n",
    "                translated = [word for word in translated if (word not in key_words)]\n",
    "                ###################Extra Section###########################\n",
    "\n",
    "                ###################Writing Section###########################\n",
    "                h.write(f\"{row+1}.\\n\")\n",
    "                h.write(f\"{' '.join(sentence)}\\n\")\n",
    "                h.write(f\"{' '.join(translated)}, \")\n",
    "                if new_flag :\n",
    "                    if season == \"\" :\n",
    "                        h.write(f\"2022 신상\\n\")\n",
    "                    else :\n",
    "                        h.write(f\"2022 {season} 신상\\n\")\n",
    "                else :\n",
    "                    h.write(\"\\n\")\n",
    "                g.write(f\"\\n-------------------{row+1}------------\\n\")\n",
    "                for key, item in buff.items() :\n",
    "                    g.write(f\"{key}\\t|\\t{item[0]}\\t|\\t{item[1]}\\n\")\n",
    "                ###################Writing Section###########################\n",
    "        g.close()\n",
    "        print(\"auto.txt has successfully saved as ./working/auto.txt\")\n",
    "    h.close()\n",
    "    print(\"new_data.txt has successfully saved as ./working/new_data.txt\")\n",
    "    print(\"Everything has completed, Go to work\")\n",
    "\n",
    "data_path=\"working/work.csv\"\n",
    "train_data=\"./train_data/reference.xlsx\"\n",
    "user_dict=\"./train_data/userdict.txt\"\n",
    "\n",
    "retrain(\"./working/new_data.txt\", \"./train_data/reference.xlsx\")\n",
    "forward(data_path, train_data, user_dict, 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating - word\n",
    "\n",
    "One hot encoding including position infos.\n",
    "\n",
    "meaning, category, position info\n",
    "\n",
    "1. give main meaning\n",
    "2. add meaning\n",
    "3. add position meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words - word - meaning - cate - \n",
    "1\n",
    "2\n",
    "3\n",
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Opn other files and save on this table too###\n",
    "###No overlap"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95cb90694882dadca987f5fb6182f3bb000d75c3a77cb8ddf9893f9283882476"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('py_selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
